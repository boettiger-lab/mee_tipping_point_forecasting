---
title: "Limits to Ecological Forecasting: Estimating Uncertainty for Critical Transitions with Deep Learning"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: mlapeyro@berkeley.edu
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu (corresponding author)
abstract: |
  1. In the current age of a rapidly changing environment, it is becoming increasingly important to study critical transitions and how to best anticipate them. Critical transitions pose extremely challenging forecasting problems, which necessitate informative uncertainty estimation rather than point forecasts. In this study, we apply some of the most cutting edge deep learning methods for probabilistic time series forecasting to several classic ecological models that examine critical transitions.
  2. Our analysis focuses on three different simulated examples of critical transitions: a Hopf bifurcation, a saddle-node bifurcation and a stochastic transition. For each scenario, we compare the forecasts from four deep learning models, Long-short Term Memory networks, Gated Recurrent Unit networks, Block Recurrent neural networks and Transformers, to forecasts from an ARIMA model and a MCMC estimated model that is given the true transition dynamics.
  3. We found that the deep learning models were able to perform comparably to the idealized MCMC model on the stochastic transition case, and generally in between the MCMC and ARIMA models on the Hopf and saddle-node bifurcation examples.
  4. Our results establish that deep learning methods warrant further exploration on the challenging class of critical transition forecasting problems. 
  
keywords:
  - Artificial Intelligence
  - Forecasting
  - Machine Learning
  - Time Series
  - Tipping points
bibliography: references.bib
nocite: |
    
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{lineno}
  - \usepackage{bbm}
  - \usepackage{setspace}
  - \linenumbers
  - \doublespacing
  
output: 
  rticles::arxiv_article:
    keep_tex: true
  word_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
library(tidyverse)
library(patchwork)
library(arrow)

bound <- function(x, percentile = 0.975) {
  dplyr::nth(x, percentile * length(x), order_by = x)
}
```



# Introduction


Forecasting plays an important and rapidly growing role in both testing our fundamental understanding of ecological processes, and informing ecological applications and conservation decision-making [@dietze_iterative_2018; @schindler_portfolio_2015].
Meanwhile, recent advances in machine learning have rapidly improved the prevalence and accuracy of short term forecasts in many fields [@kao_exploring_2020; @lyu_lstm_2020; @du_multivariate_2020]. 
Will these emerging methods improve the capacity for forecasts in ecological systems as well?
Ecological dynamics are notoriously complex, with uncertainty and non-linearity playing critical roles [@Boettiger2018; @hallett_why_2004; @Ovaskainen2010].
These challenges are nowhere more evident than in _critical transitions_, sudden shifts in the states or patterns of ecosystem dynamics that are more important and more difficult to predict than gradual changes.
Here, we examine several of the best-known examples of critical transitions in ecological systems.
We evaluate the most promising machine learning methods for probabilistic forecasts relative to traditional statistical and mechanistic approaches applied to several classic models in ecology.

In this paper, we focus on the task of producing quantitative, probabilistic forecasts reflecting the possible distribution of future states, as frequently called for in ecological research [@clark_ecological_2001; @dietze_iterative_2018].
Such forecasting tasks may arise whenever a manager is interested in knowing the future states of a system: such as setting future catch quotas for a fishery or adjusting eradication effort for an invasive species.
It is important to distinguish this objective from the extensive previous literature on “early warning signs” of critical transitions, as reviewed in @scheffer_early-warning_2009, which has sought to answer only a categorical question: is the system approaching a critical transition?
Recent work such as @bury_2021 has introduced ML methods to consider classification of this transition in four possible categories (Hopf, saddle-node, transcritical, or no bifurcation) rather than two (bifurcation or not).
These are important results with considerable promise [@lapeyrolerie_teaching_2021], but which nevertheless address a very different question using very different methods. 
Early warning signals only predict "a big change may be coming soon" -- they do not try to forecast when or how big.
As we shall see, there is good reason to focus on that more modest, qualitative objective when faced with systems that might produce critical transitions.
Here, we examine the more ambitious questions of forecasting when and how much change: or more precisely, of making probabilistic forecasts of all future states over a given time horizon.



# Materials and Methods

<!-- some more verbose bits about bifurcation models probably belongs in discussion instead -->
We will focus the analysis on several different forecasting scenarios based around two classic models in population ecology: 
Robert May's consumer-resource model [@May1977], and the Nicholson-Bailey parasitoid-host model [@nicholson_balance_1935]. 
Though these models may appear simple when measured against high-dimensional and parameter rich models found in some management contexts such as fisheries, they can exhibit rich nonlinear dynamics and provide greater capacity to generalize [@Levins1966; @Getz2018]. 
These textbook models have been well studied and form the basis of half a century of research in ecology, including much recent work on topics such as resilience and tipping points which has had important theoretical and practical management outcomes [@Folke2004; @Fischer2009; @Polasky2011].
May's model exhibits alternative stable states. 
In this one-dimensional model, transitions between these states can occur due to intrinsic stochasticity, external forcing, or the gradual environmental change that results in a catastrophic saddle-node bifurcation and generates hysteresis. 
The Nicholson-Bailey model is a two species model which contains a supercritical Hopf bifurcation, a non-catastrophic bifurcation which either creates or destroys a limit cycle -- a stable oscillatory pattern. 

Assessing the accuracy of forecasting methods in the face of such bifurcation dynamics is a particularly important question for ecological systems and global environmental change problems.
Bifurcations represent the kind of non-linear responses complex systems can make as the result of slowly changing parameters.
This can create a particularly challenging forecasting task when such transitions have not been previously observed in the same system, requiring the forecast to anticipate dynamics for which there are no analogs in the historical data. 
Forecast skill under such no-analog conditions may be particularly relevant to ecological forecasting in the context of global change [@Williams2007].

We provide fully reproducible coded examples in R and Python for performing, scoring, and visualizing each of the forecasts considered here.
After significant time spent considering alternative frameworks, we have emphasized those which best met our requirements for performance, ease-of-use, flexibility, and support for the latest probabilistic machine learning models for forecasting. 
Most of our forecasts use the `darts` framework, a sophisticated and well documented Python library with support for a wide range of methods.
Our model-based MCMC forecasts use the `greta` framework, a R library that uses Python-based `tensorflow` probability to achieve better performance.
While Python-based frameworks currently have the edge in performance and access modern ML algorithms, they lag behind in attention to statistical issues such as the computation of strictly proper skill scores.

Our examples of scoring and visualization will rely on a collection of R packages, in particular, `scoringRules` for the efficient calculation of Continuous Ranked Probability Score (CRPS) and logarithmic probability (Logs) scores for forecast ensembles [@gneiting_strictly_2007].
Following popular conventions, we express both skill scores in error-orientation, that is, larger values indicate worse skill (higher degree of error).

We expect greater convergence between methods available in R and Python in the future, as already illustrated in the example of `greta`.
Complete code for all examples presented here can be found at https://github.com/boettiger-lab/mee_tipping_point_forecasting.


```{r figure1, fig.cap="Forecast scenarios. A. The Hopf bifurcation: a stable node develops into a limit cycle which gradually grows larger in this predator-prey model. B. The saddle-node bifurcation in a single species. C. The stochastic transition in a single species.  Plots show historical data used to train the algorithm in purple, and replicate simulations of the true dynamics ('future ensemble') in yellow. Note how the characteristic time for the critical transition varies across the transitions. We will examine forecasts of various models (Fig 2) which will each produce probablistic forecast distributions (blue, Fig 3-5) seeking to match the true future ensemble (yellow) as closely as possible.", fig.height=6}
hopf <- read_csv("../greta/forecasts/simulation=hopf/hopf_increasing.csv.gz")
panelA <- hopf |> 
  mutate(type = as.factor(type)) |>
  filter(type %in% c("historical", "true"), i < 10) |>
  rename(species = variable) |> 
  mutate(species = forcats::fct_recode(species, host = "H", parasitoid = "P")) |>
  bind_rows(tibble(i=1,t=1,type="predicted", variable="H", value=NA)) |> #placeholder
  mutate(type = forcats::fct_recode(type, "future ensemble" = "true", "forecast" = "predicted")) |>
  ggplot(aes(t, value, group=interaction(species, i, type), col=type, lty=species)) + 
  geom_line(show.legend = TRUE, alpha = 0.7) +
  theme_bw() +  
  scale_color_viridis_d(guide="none") +
  ggtitle("A. Hopf bifurcation")+
  theme(legend.position="top")

sn <- read_csv("../greta/forecasts/simulation=saddle/saddlenode.csv.gz")
panelB <- sn |>
  mutate(type = as.factor(type)) |>
  filter(type == "historical", i == 1) |>
  bind_rows( filter(sn, type=="true")) |>
  bind_rows(tibble(i=1,t=1,type="predicted", variable="N", value=NA)) |> #placeholder
  mutate(type = forcats::fct_recode(type, "future ensemble" = "true", "forecast" = "predicted")) |>
  ggplot(aes(t, value, col = type, group=interaction(i,type), lty=type)) +
  geom_path(alpha=0.5, show.legend = FALSE) +
  scale_color_viridis_d() +
  theme_bw() + 
  ggtitle("B. Saddle-node bifurcation")

stoch <- read_csv("../greta/forecasts/simulation=stochastic/stochastic.csv.gz")
panelC <- stoch |>
  filter(type == "historical", i == 1) |>
  bind_rows(filter(stoch, type=="true", i < 50)) |> 
  bind_rows(tibble(i=1,t=1,type="predicted", variable="N", value=NA)) |> #placeholder
  mutate(type = forcats::fct_recode(type, "future ensemble" = "true", "forecast" = "predicted")) |>
  ggplot(aes(t, value, col = type, group=interaction(i,type), lty=type)) +
  geom_path(alpha=0.5) + 
  scale_color_viridis_d() +
#  scale_color_grey(end=0.6) + 
  theme_bw() + 
  ggtitle("C. Stochastic transition") +
  theme(legend.position="bottom")


panelA / panelB / panelC

```

## Scenario 1: Hopf Bifurcation

The Nicholson-Bailey model describes a predator-prey dynamic for the relationship of a host species and an obligate parasitoid, originally used to model the population dynamics of blowflies (_Lucilia cuprina_) [@nicholson_balance_1935; @nicholson_outline_1954; @nicholson_compensatory_1954].
We consider the form which includes density dependence in the host species, and allow for environmental stochasticity,

\begin{align}
H_{t+1} &= H_t \exp \left( r \left(1 - \tfrac{H}{K_t}\right) - c P_t + \eta_{H,t} \right) \\
P_{t+1} &= H_t \exp \left( r \left(1 - \tfrac{H}{K_t}\right) \right)  \left(1 -  \exp \left(- c P_t + \eta_{P,t}\right) \right) \\
K_{t+1} &= K_t + \delta
\label{hopf}
\end{align}

Where $H_t$ is the population density of the host species at time $t$, (in arbitrary units) and $P_t$ the population density of the parasitoid.
The time step is defined by the generation time of the parasitoid, which is about two weeks in the case of Nicholson's blowflies [@nicholson_outline_1954].
Following @dakos_methods_2012, we further allow the carrying capacity of the host, $K$ to slowly increase at a linear rate, which drives a supercritical Hopf bifurcation as $K$ becomes sufficiently large. 
In a Hopf bifurcation, a stable node starts an oscillatory pattern which grows in amplitude as the bifurcation parameter continues to increase.
In this model, the Hopf bifurcation is dubbed 'supercritical' as it creates a stable limit cycle instead of an unstable one.
This example illustrates one of the many kinds of challenges which nonlinear phenomena pose to forecasting: the "historical" data prior to the bifurcation never exhibit the cyclical dynamics of growing amplitude that will emerge after the bifurcation occurs.
If we had used a purely deterministic model, the dynamics would be constrained to a single stable point, corresponding to a slowly changing steady-state population size of host and parasitoid populations.
However, stochasticity in this case acts as a source of some additional information about the dynamics, as the noise excites quasi-cycles which are visible in the irregular oscillations that appear significantly prior to the emergence of true limit cycles which follow the bifurcation [@boettiger_noise_2018].
Examples use the following parameters: $H_0 = 9$, $P_0 = 1$, $r = 0.75$, $c = 0.1$, $K_0 = 14$, $\delta = 0.08$, $\sigma_H = 0.02$, $\sigma_P = 0.02$. 


## Scenario 2: The saddle-node bifurcation

A yet more difficult forecasting scenario is created by the saddle-node bifurcation.
May's consumer-resource model is an one-dimensional model describing the growth of a 'resource' population (e.g. herbivore) which is grazed by a consumer [@may_population_1979].
As in the Nicholson-Bailey model, in the absence of that predation, the resource population density grows under a density-dependent pattern described by a logistic function.
The resource population is also grazed by a consumer at a rate given by a Holling type III s-curve (typically used to model handling time). 
For a certain range of parameter choices, this model supports alternative stable state dynamics, and has been identified and employed in explaining alternative stable state dynamics in a broad range of ecological and socio-ecological systems [@scheffer_catastrophic_2001]. 

\begin{align}
N_{t+1} &= N_t + r N_t \left(1 - \frac{N_t}{K} \right) - \frac{h_t N_t^2}{s^2 + N_t^2} + \eta_t \\
h_{t+1} &= h_t + \alpha \\
\eta_t &\sim \mathcal{N}(0, \sigma)
\end{align}

If the environment slowly alters one of the parameters (say, the encounter efficiency, `h_t`, in our formulation), one of the stable nodes moves closer and closer to the unstable saddle point, leading to a bifurcation that destroys the stable state, leaving the system to suddenly transition to the alternative stable state.
Saddle-node bifurcations (also known as fold bifurcations) also create a phenomenon known as hysteresis, where it is not sufficient to restore the environment to the previous parameter values to recover the previous state. 
Unlike the supercritical Hopf bifurcation which exhibits a continuous transition from a stable node to a small limit cycle that then grows, the saddle-node transition is a discontinuous or so-called 'catastrophic' bifurcation.
Due both to this sudden, catastrophic nature of the transition and the difficulty in reversing the shift after it has occurred, saddle-node bifurcations have been the subject of intense study.

Tipping point dynamics have long been identified as an important but difficult challenge for forecasting [e.g. @Scheffer2001; @Folke2004]. 
Much effort in the ecological literature so far has focused on identifying any 'early warning signs' that a catastrophic bifurcation might occur at all [@scheffer_early-warning_2009; @scheffer_anticipating_2012] rather than more ambitious attempts to provide quantitative probabilistic forecasts of the likely distribution of waiting times before such a transition occurs.
Tipping points resulting from saddle-node bifurcations have been demonstrated in examples ranging from laboratory microcosms [@Dai2012; @Dai2015] to whole-ecosystem experiments [@carpenter_early_2011], and postulated as a model for global change [@Barnosky2012]. 
Examples use the following parameters: $r = 1$, $K = 1$, $s = 0.1$, $h_0 = 0.15$, $\alpha = 0.000375$, $\sigma = 0.02$, $N_0 = 0.75$.


## Scenario 3: The stochastic transition

Perhaps the most difficult of all events to predict are those in which large transitions are predominately driven by a random component. 
An example of such a transition event is possible to observe in May's consumer-resource model, in which a stochastic term occasionally results in a transition between alternative stable states.
In such cases, no forecast can precisely predict when a transition will occur, but it is nonetheless possible to deduce the correct distribution of waiting times knowing the correct model. 
In the case of small noise, transitions are Poisson distributed, such that the distribution of waiting times is roughly exponential [e.g. @kampen_stochastic_1992], though post-hoc the trajectories of such transitions can be mistaken for saddle-node transitions [@boettiger_early_2012].
To consider such cases, we will again use May's alternative stable state model, though this time leaving all parameters fixed. 

In this context, predicting the probability of a transition in the future based solely on observations prior to a transition occurring is essentially impossible without additional information constraining the model estimate, as such data is equally consistent with infinitely many models or parameter choices which share the same local linearization about the stable point.
Unlike the saddle-node bifurcation, there is no slowly warping potential basin which can be detected to inform estimates. 
Thus, in this scenario, rather than considering the problem of predicting the future evolution of a single time series based only on its historical values, we consider an alternative framing of the task:
we imagine our forecaster has access to historical data from one or more comparable systems which includes a previous stochastic transition event.
Based on this data, our forecaster seeks to identify the distribution of expected transition times for analogous systems starting from the same initial condition. 
This parallels actual practice in which researchers would draw on previous examples of stochastic transitions in a system - lake-ecosystem shifts, disease emergence, changing fire regimes, [@Scheffer2001; @Folke2004].
(Note that such stochastic transitions between alternative stable states can also create oscillatory-like dynamics when stochasticity is sufficiently high enough to drive repeated transitions from one attractor to the other and back again. In such cases, it might be reasonable to estimate a strictly forward-looking forecast of a single system, predicting the distribution of these transitions.)
Model definition is the same as May's model for the saddle node with fixed parameter $h$, values: $r = 1$, $K = 1$, $s = 0.1$, $h_0 = h = 0.26$, $\alpha = 0$, $\sigma = 0.02$, $N_0 = 0.55$.


## Selecting timescales
In each scenario,  t = 0 is the start time of the training data, while the length of training data and forecast horizon (with ensembles sampled from the true distribution) is illustrated in Fig 1. 
For the Hopf bifurcation, forecast begins at t=100 and extends to t =200, for the saddle node, forecast begins at t=250 and extends to t = 500, for the stochastic transition, both training data and forecasting tasks begin at 0 and extend to t = 250. 
While much attention is often paid to the number of data points in training or testing data, it is essential to realize that these are only meaningful relative to the specific process in question. 
Thus, in each case we have selected these time intervals to focus on the dynamical process in question, which unfolds at a different rate and tempo in each scenario. 
For instance, if the stochastic scenario was restricted to a much shorter timescale used in the Hopf case, few replicate simulations would experience a transition at all. 
If it were made much longer, most of the timeseries would be spent post-transition. 
Likewise, if the forecast for the Hopf scenario was extended much further into the future under the current parameterization, the system experiences a homoclinic bifurcation at which the population collapses to 0. 
Using different length timescales allows us to consider the three different forecasting tasks illustrated in Fig 1 that focus around predicting the critical behavior, rather than predicting long periods of relative stasis.
These three critical transitions are fundamentally different processes, there is no perfect apples-to-apples parameterization for each that allows the transition to unfold in a way that gives precisely the same time windows.


## Method group 1: Markov Chain Monte Carlo

As a reference case, we consider forecasts produced by MCMC estimates of model parameters, _given the true model_.
This represents an idealized case where the nature of the underlying process is known precisely.
Uncertainty comes from parameter estimates and intrinsic stochasticity specified in the model, but does not reflect any uncertainty in our knowledge of the model structure. 
Alternative model structures, even when capable of producing the same nonlinear phenomena (i.e. the same bifurcations) will give very different forecasts. 
Even alternative prior distributions of the parameters will generally yield alternate forecasts, as likelihood ridges are common to nonlinear models.
Thus, this case represents a theoretical upper bound for the performance of forecasts by techniques which do not make such strong assumptions about the underlying processes.


## Method group 2: Statistical models (ARIMA)

<!-- Key distinction here from MCMC above is not on how model parameters are estimated, but rather on approaches that aren't making strong mechanistic assumptions about process, but are merely phenomenological type models
-->

We present forecasts produced by ARIMA models as the model-free analogs to the forecasts made using parameter estimation with MCMC.
Since ARIMA models make the assumption that the future will resemble the past via ARIMA's auto-regressive and moving average components [@hyndman_forecasting_2018], these models are not well-suited for problems with complex bifurcation dynamics.
Thus, ARIMA-based forecasts should be treated as a lower bound for the performance of non-mechanistic models.
In contrast to inference with MCMC, uncertainty with ARIMA models is estimated directly from the learned parameters [@hyndman_forecasting_2018].
Since ARIMA is a commonly encountered method, we will refer readers to @hyndman_forecasting_2018 for further discussion.


## Method group 3: Machine Learning models

Over the past decade, deep learning has become very popular for a broad range of challenging time series prediction problems [@makridakis_statistical_2018].
Deep learning models are often used to make point forecasts, but for their application to ecological time series, it will often be necessary to use multi-step, probabilistic forecasts. 
For all the deep learning models in this study, we use the same general process.
Each machine learning model is trained on one time series drawn from the three scenarios described previously.
For the Hopf and saddle node cases, these time series consist of the period leading up to the bifurcation.
A critical transition is, however, included in the training set for the stochastic transition case.
Each model is trained to learn the parameters of a Laplace distribution for every time step in the forecast horizon.
To produce a forecast, we input a time series into a model, then we draw samples from the distributions that were learned during training.

A major nuisance with deep learning methods is their instability to hyperparameters and initialization seeds [@madhyastha_model_2019].
We found that for the same set of hyperparameters, we could produce starkly different forecasts if we trained the same model with different initialization seeds.
One explanation for this instability is that machine learning models often get stuck on the local optima of loss surfaces [@madhyastha_model_2019]. 
Another likely cause is that machine learning models commonly overfit the training data [@mehta_high-bias_2019].
Across deep learning, overfitting is a fundamental issue, arising from neural networks being highly overparameterized [@dar_farewell_2021]. 
With so many parameters, deep learning models tend to have high variance and thus overfit the training data, a consequence of the bias-variance trade-off common across statistics and machine learning [@mehta_high-bias_2019].
One frequently used method to reduce overfitting is K-fold cross validation [@raschka_model_2020], but this approach cannot be effectively employed when there is one or few time series in the training set.
To remedy the instability problem, we use an ensemble-based method, wherein each ML forecast is the union of forecasts from 5 individual models that were trained with different initialization seeds.
We found this simple ensemble technique to be an effective way to improve generalizability in the limited data regime.

Recently, it has become established that using memory or attention-based neural networks, and an encoder-decoder architecture is crucial for improving forecasting performance on time series data [@kao_exploring_2020; @lyu_lstm_2020; @du_multivariate_2020].
Herein we will provide some background on what these machine learning methods are and their benefits.

### Recurrent Neural Networks

Recurrent neural networks are the predominant memory-based deep learning method.
Recurrent neural networks differ from feed-forward neural networks in that a recurrent neural network provides feedback to itself between time steps [@sherstinsky_fundamentals_2020].
By providing self-feedback, recurrent neural networks are able to retain information from previous time steps and thus learn temporal dependencies.
However, a standard recurrent neural network is unwieldy to train because of the vanishing and exploding gradient problem [@pascanu_difficulty_2013], so there have been specialized neural network architectures designed to avoid these gradient problems.
Long Short-term Memory and Gated Recurrent Units Networks are considered to be the state of the art recurrent neural networks that address exploding and vanishing gradients [@chung_empirical_2014].
These methods avoid gradient problems by regulating the self-feedback via gates which perform operations on the feedback signal -- see @chung_empirical_2014 for more details.
While GRU's and LSTM's commonly outperform standard RNN's, it is difficult to anticipate whether GRU's or LSTM's will be best suited for any time series problem [@chung_empirical_2014], so we investigate both methods.

### Transformers

The Transformer is a state of the art ML architecture that is able to model long and short term dependencies on sequence to sequence tasks [@vaswani_attention_2017].
Transformers use a mechanism called self-attention which interrelates different positions of the input sequence in order to find an informative representation of the input sequence [@vaswani_attention_2017].
For example, if given a sentence, a transformer could learn the contextual relationship between a subject and a direct object, but a recurrent neural network would process all the words as one phrase.
Because of self-attention, Transformers do not need to process data sequentially and thus can be parallelized, offering significant computational advantages [@vaswani_attention_2017].
The Transformer is likely to be a foundational method for future AI research [@bommasani_opportunities_2021], so we considered it critical to investigate Transformers in this study.

### Encoder-Decoder

Encoder-decoder architectures have been shown empirically to excel on sequence to sequence tasks [@aitken_understanding_2021].
Encoder-decoders work by processing the input sequence into a fixed-length vector then decoding the fixed-length vector to the predicted output sequence.
It is thought that by encoding the input sequence to a vector, encoder-decoders find informative representations of the input sequence that make the prediction task much easier [@sutskever_sequence_2014].
Note that it is possible to use any type of neural network as the encoder and the decoder, but it is most common to use recurrent neural networks or networks with attention mechanisms [@aitken_understanding_2021].
Of the models that we present, Block RNNs and Transformers have encoder-decoder-based architectures.

<!--
Basic typology of machine learning methods, how we distinguish "ML" from statistical methods (i.e. bias-variance tradeoff; no criteria to prove that the algorithm produces an unbiased estimate of a given summary statistic)

Stuff about how most machine learning techniques provide only point forecasts
-->

## Forecast skill: strictly proper scores

To compare forecasts, we focus exclusively on metrics of forecast skill which satisfy the property from @gneiting_strictly_2007 of a strictly proper score.
This ensures the very desirable behavior that no probabilistic forecast $Q(x,t)$ can have a score as high as the score of the true process $P(x,t)$ on average. 
In other words, while it is possible for any of the models considered to _overfit_ the data against which they are trained, i.e. have a higher likelihood than the true process, it is not possible for these models to overfit the data against which they are scored.
It is worth noting that this property applies specifically to probabilistic forecasts and not point forecasts. Not all common metrics often used to compare forecasts are strictly proper -- such as the average root-mean-square error or the average absolute error. <!-- MFL: hmm I am not completely following this point here. CB: added some examples of non-strictly-proper scoring metrics. For instance, {darts} has a whole 'metrics' class, which does not include any strictly proper metrics (because ML researchers don't care about statistical theorems?) -->
Concerns about over-fitting arise in most types of model estimation and are a particularly acute concern to machine learning methods due to the bias-variance trade-off [@mehta_high-bias_2019]. <!-- MFL: I don't think I nailed the overfitting part in ML section but we probably want to reference this here-->
This makes the use of strictly proper scoring especially relevant in assessing machine learning predictions.

Not even all strictly proper scores will agree on the same relative ranking between forecasts.
We will focus on two of the most common such skill metrics, CRPS score and log probability score (negative log likelihood) [e.g. see @gneiting_strictly_2007; @gneiting_probabilistic_2014].
We define these scores explicitly in Equations 7-8, where $F$ and $f$ respectively correspond to the cumulative distribution function and probability density function of the forecast; and, $y$ denotes an observation.
Of the two metrics, the logs probability score puts a much a greater penalty on unexpected observations than CRPS, and may be more suitable when the occurrence of unexpected events incurs a particularly high cost.
Note that while the minus log-likelihood can be negative for sufficiently high probability densities, we use a fixed scalar shift of logs score to ensure the log skill score is strictly positive, which facilitates visualization without impacting relative rankings.

\begin{align}
\text{LogS}(F, y) &= - \log{f(y)} \\
\text{CRPS}(F, y) &= \int (F(z) - \mathbbm{1}\{y \leq z\})^2 \, dz
\end{align}

# Results

We examine forecast skill for each of the six forecasting methods (MCMC, ARIMA, block-RNN, GRU, LSTM, and Transformer) in each of our three scenarios (Hopf bifurcation, saddle-node bifurcation, and stochastic transition).
In addition to these cases, we also consider an "ensemble model", generated by drawing from the distribution of all models except the MCMC model. 
Such ensemble techniques can better reflect uncertainty than relying on any single method [@gneiting_weather_2005].
For simplicity, we consider the unweighted case, where each model is represented equally in the ensemble.
Using model-based simulations allows us to examine performance against multiple (n=100) replicates of the "true" process, which further helps identify differences that may occur solely due to chance. 
By taking the true model structure as given, MCMC methods can be used to determine a theoretical limit of forecasting skill.
Note that in both bifurcation scenarios, future dynamics will visit states never previously observed in the historical data that was used to train each of the methods (e.g. very small population sizes). 
This no-analog aspect of forecasting bifurcation dynamics means that even with many sample points in the training data _and_ perfect knowledge of the true model structure, posterior distributions of parameter values are still influenced by the choice of priors. 


```{r}
scores <- arrow::open_dataset("../scores", format="csv") |> collect()
# shift logs score to strictly positive, non-infinite values so we can use a log-scale
shift <- scores |> 
  filter(!is.infinite(logs)) |> 
  mutate(logs = logs - min(logs) +.01) |>
  pivot_longer(c(logs, crps), 
               names_to = "metric", 
               values_to = "score")
```


```{r figure2, dev="png", dpi=300, dev.args = list(type = "cairo-png"), out.width="400px", out.height="300px"}
#| fig.cap="Overall distribution of skill scores across models, including an ensemble of methods.
#|  Smaller scores are better (indicating smaller errors).
#|  Black bars indicate means, dots indicate all individual predictions over time and replicate 'true'
#| simulations of the given scenario."
shift |> 
  ggplot(aes(forecasting_model, score, color=forecasting_model)) + 
  geom_jitter(alpha = 0.06, show.legend = FALSE, shape=".") + 
  stat_summary(fun = mean, shape="|", show.legend = FALSE, col="black") +
  scale_x_discrete(limits = rev) +
  coord_flip() + 
  facet_grid(simulation ~ metric, scales = "free") + 
  scale_y_log10() +
  theme_bw() +
  theme(axis.title.y = element_blank())
```


Overall forecasting skill scores for each model across all three scenarios are summarized in Fig 2.
Average scores (black lines) hide wide variation in forecast skill.
Generally, ML performance tends to be bracketed between MCMC (essentially the theoretical optimum), and the statistical ARIMA model, though sometimes performing worse than ARIMA or better than MCMC.
Under scenarios with alternative stable states (saddle and stochastic), the distribution of scores is often bimodal for ML models, though not MCMC. 
The ML ensemble model often performs as well as the best ML model on average.
Note that a wide prediction of uncertainty does not mean a wide range in the score skill -- for instance the ensemble model which has the widest array of outcomes often has a relatively tight distribution of score, especially in logs skill.
This reflects the relative contributions of accuracy and uncertainty as components in the forecasts.
Most ML scores are comparable to MCMC skill except for the scenario of the saddle-node bifurcation, where all other models are much worse.
To get a deeper understanding of these general patterns, we now turn to examine each of the forecast distributions themselves in comparison to the future ensemble produced by the true generative process model, Fig 3-5.


```{r}
forecasts <- read_parquet("data/forecasts.parquet") |>
  filter(forecasting_model != "ml_ensemble") |> 
  bind_rows(tibble(type= c("future ensemble", "historical"))) |>
  mutate(type = forcats::fct_recode(type, "future ensemble" = "observed", "forecast" = "predicted"))

observations <- read_parquet("data/observations.parquet") |> 
  mutate(type = forcats::fct_recode(type, "future ensemble" = "observed", "forecast" = "predicted"))


hopf_obs <- observations |> 
  mutate(variable = as.character(variable)) |>
  filter(iter %in% 1:15, simulation == "hopf", variable !="X")
saddle_obs <- observations |> filter(iter %in% 1:15, simulation == "saddle")
stochastic_obs <- observations |> filter(iter %in% 1:15, simulation == "stochastic")

```

```{r figure3, fig.height=7}
#| fig.cap="Forecasts of the Hopf bifurcation under each model, compared to 15 realizations of the true model. 
#|  The bifurcation occurs soon after forecasting period begins, 
#| leading to progressively larger and larger oscillations. 
#|  Prior to the bifurcation, pseudo-cycles are visible in the training data due to stochastic excitations. 
#|  Following the bifurcation, stochasticity blurs the oscillatory pattern across replicate simulations. 
#|  Only last 25 time points of training data are shown."

forecasts  |> 
  filter(iter %in% 1:15, simulation == "hopf", variable !="X") |> 
  bind_rows(tibble(type= c("future ensemble", "historical"), forecasting_model="MCMC", variable="host")) |>
  mutate(type = factor(type,levels=c("historical","forecast", "future ensemble"))) |>
  ggplot(aes(x=t, y=value, group = interaction(iter, variable), col=type)) + 
  geom_line(alpha=0.1) +
  geom_line(alpha=0.1, data = hopf_obs) +
  geom_line(data = filter(hopf_obs, type=="historical")) +
  facet_grid(forecasting_model ~ variable, scales = "free") +
  theme_bw()  + 
  scale_color_viridis_d() +
  coord_cartesian(ylim = c(0,15), xlim=c(75, 200)) + 
  ggtitle("Hopf bifurcation")
```



Forecasts of the Hopf bifurcation (Fig 3) are roughly comparable across the phenomenological models (ARIMA and machine learning models).
All models are trained using 100 time points drawn from the period of time prior to the onset of the Hopf bifurcation, which leads to a stable limit cycle that gradually grows in magnitude.
Most models predict a roughly constant mean with a spread roughly equal to that created by the stochastic oscillations around the stable node as seen in the training data prior to the bifurcation.
Notably, the GRU method picks up the oscillatory nature of the dynamics, despite the fact that no true oscillations were yet present in the training data.
However, like the other ML models, it fails to predict the growing amplitude of those oscillations.
Having access to the true model structure, the MCMC model alone predicts the transition into a pattern of oscillations which grows over time, though it tends to overestimate the amplitude of those oscillations initially.
Despite this, overall all methods score comparably in CRPS score (Fig 2) with most ML methods actually out-performing the MCMC score on average (Fig 5), albeit with much greater variation in individual scores.
A clearer picture can be seen by looking at these skill scores over time (Fig 6-7), which show that MCMC is initially performing worse (over-predicting variance) but as oscillations grow further, it starts outperforming the more stationary forecasts of the ML models.


<!-- would be interesting to revisit this with lower stochasticity such that the cyclic behavior is more crisp, and the quasi-cycles in the training data are smaller-->

```{r figure4}
#| fig.cap = "Forecasts of the saddle-node bifurcation under each model, compared to 15 realizations of the true model. 
#|  Training data proceeds the bifurcation, making accurate prediction without knowledge of the underlying model very difficult. "


forecasts |> 
  filter(iter %in% 1:15, simulation == "saddle") |> 
  bind_rows(tibble(type= c("future ensemble", "historical"), forecasting_model="MCMC", variable="host")) |>
  mutate(type = factor(type,levels=c("historical","forecast", "future ensemble"))) |> 
  ggplot(aes(x=t, y=value, group = interaction(iter, variable), col=type)) + 
  geom_line(alpha=0.1) +
  geom_line(alpha=0.1, data = saddle_obs) +
  geom_line(data = filter(saddle_obs, type=="historical")) +
  facet_wrap(~forecasting_model, scales = "free") +
  theme_bw()  + 
  scale_color_viridis_d() +
  coord_cartesian(ylim = c(0,1)) + 
  ggtitle("Saddle-Node bifurcation")
```

The saddle node bifurcation proves even more difficult for most methods (Fig 4).
Only the MCMC model anticipates the sharp transition to an alternative state, though this behavior is more baked into the method from the start which takes the true model structure as given. 
Even accurate estimation of the MCMC requires slightly informative priors, though still broad enough to reflect a wide range of possible outcomes. 
Two ML methods -- Block RNNs and Transformers -- resemble a naive prediction extrapolating the last observed state, failing even to reflect the slow downward trend of the training data. 
LSTM indicates greater uncertainty, while GRU shows very large variability which spans the alternative stable state range. 
With additional tuning, better performance may be possible for these ML models. 
The selected ARIMA model reflects wide uncertainty that is nevertheless not broad enough to span the alternative stable state.
Consequentially, the MCMC estimate easily outperforms the ML models (Fig 2).

```{r figure5}
#| fig.cap = "Forecasts of the stochastic transition under each model, compared to 15 realizations of the true model. 
#|  In contrast to the other challenges, this case considers the prediction of replicate systems starting from the same initial condition, rather than forecasting the future evolution of the model after the stochastic transition has already occurred."
forecasts |>
  filter(iter %in% 1:15, simulation == "stochastic") |> 
  bind_rows(tibble(type= c("future ensemble", "historical"), forecasting_model="MCMC", variable="host")) |>
  mutate(type = factor(type,levels=c("historical","forecast", "future ensemble"))) |> 
  ggplot(aes(x=t, y=value, group = interaction(iter, variable), col=type)) + 
  geom_line(alpha=0.2) +
  geom_line(alpha=0.2, data = filter(stochastic_obs, type=="future ensemble")) +
  geom_line(data = filter(stochastic_obs, type=="historical")) +
  facet_wrap(~forecasting_model, scales = "free") +
  theme_bw()  + 
  scale_color_viridis_d() +
  coord_cartesian(ylim = c(0,0.8)) + 
  ggtitle("Stochastic transition")
```

Machine learning methods do markedly better on the stochastic transition scenario than in the two bifurcation scenarios (Fig 5).
This occurs because the training data includes the transition phenomenon of interest.
All ML models accurately capture the dynamics of a sharp transition between alternative stable states -- a dynamic the statistical ARIMA model entirely fails to reflect.
Stochastic transition events should be approximately exponentially distributed, as seen in the wide range of waiting times for transitions to occur in replicates of the true 'observed' process (Fig 5).
Transformer and Block RNN distribution times are much more concentrated, while again GRU and especially the LSTM do a better job reflecting the uncertainty in range of transition times. 


Examining patterns in the scores over time (Fig 6-7) provides a more nuanced understanding of the forecast dynamics than aggregate scores alone (Fig 2).
In the Hopf bifurcation, CRPS scores get worse over time across all methods, including the MCMC forecasts.
In the saddle node bifurcation and stochastic transition, the same pattern holds somewhat more dramatically for non-MCMC forecast, while MCMC scores are worst in mid-range.
Comparing CRPS scores to logs score also emphasizes the relative role of uncertainty: for instance, the MCMC scores for the Hopf bifurcation get steadily worse under CRPS but not under logs score. 
A relatively sharp transition can be seen under both MCMC scores on the Hopf bifurcation once the magnitude of the oscillations exceeds the variance created by mere stochasticity: the MCMC model no longer over-estimates the spread of the data, while the ML models now underestimate that variation. 
CRPS scores for stochastic transitions exhibit a distinct two-branch pattern, with scores for a given replicate being either very high (poor skill) or very low, reflecting whether the individual 'true' replicate matches the mean state predicted by the forecast or the other state. 
Logs skill score may be a better measure in this context, where correctly capturing the uncertainty in the forecast means that this bi-modal structure in scores can be avoided entirely, e.g. by the MCMC predictions. 
The forecast-skill-over time plots illustrate different reasons for the bi-modal distribution in skill seen for the saddle-node and stochastic transition scenarios in Fig 2 respectively: in the case of the saddle node, the two modes are distinguished by time-horizon: short term forecasts are relatively accurate, and longer term forecasts (i.e. after the catastrophic transition) are poor.
In the case of the stochastic transition model, the two modes are not structured by horizon but by replicate, with some replicates having transitioned and others still in the original state.


```{r figure6}
#| fig.cap="CRPS scores over time for each scenario.
#|  Each line represents the scores against a replicate of time series observations from 'true model'. 
#|  Patterns show forecast skill generally get worse over time -- gradually 
#| in the case of Hopf bifurcation or suddenly in response to the saddle-node bifurcation. 
#|  The MCMC shows a sharp improvement in forecast skill over time as it passes the transition window. 
#|  In the stochastic transition context, this creates two branches, where high values indicate periods of 
#| time when the forecast predicts the wrong state and lower branch indicates the correct state."

shift |> 
  group_by(simulation,forecasting_model, metric) |> 
  mutate(t = t - min(t)) |> ungroup() |> 
  filter(metric == "crps", t<100) |> 
  ggplot(aes(t, score, col = forecasting_model, fill=forecasting_model)) + 
  geom_line(aes(group=iter), alpha = 0.05, show.legend=FALSE) +
  facet_grid(simulation ~ forecasting_model, scales="free") + 
  theme_bw()  + ggtitle("CRPS scores over time")
```

```{r figure7}
#| fig.cap="Logs skill score over time. 
#|  Forecasts which underestimate uncertainty do substantially worse in logs score than in CRPS score. 
#|  Comparing this panel to those in Fig 6 highlights scenarios that most often underestimate uncertainty. 
#|  Generally, MCMC performs better relative to other models under this metric than it does under CRPS, 
#| reflecting the bias-variance tradeoff of machine learning."

shift |> 
  group_by(simulation,forecasting_model, metric) |> 
  mutate(t = t - min(t)) |> ungroup() |> 
  filter(metric == "logs", t<100) |> 
  ggplot(aes(t, score, col = forecasting_model, fill=forecasting_model)) + 
  geom_line(aes(group=iter), alpha = 0.05, show.legend=FALSE) +
  facet_grid(simulation ~ forecasting_model, scales="free") + 
  theme_bw() + scale_y_log10() + ggtitle("Logs skill score over time")
```


# Discussion

Ecological systems have long been acknowledged as complex, due not only to the immense span of dimension and scale such processes involve, but also the frequency of emergent and non-linear phenomena such as stochastic resonance, including bifurcations, tipping points, and hysteresis examined here.
Calls for increased forecasting efforts from ecologists frequently reference the role of changing climate and other anthropogenic change, which raise the challenge of prediction in no-analog environments, anticipating ecosystem responses to conditions that have not been previously observed [@clark_ecological_2001; @dietze_iterative_2018].
This motivates the question, "What methods will be most reliable in the face of unobserved conditions?"

In this paper, we carry out an initial exploration on how deep learning methods can perform on predicting critical transition events.
We compare the ability of several cutting edge machine learning approaches against statistical and process-based models, and show that deep learning methods are generally able to strike a middle ground between what we consider as acceptable and ideal case forecasting methods, ARIMA and MCMC-based parameter estimation respectively.
Although most ML-based forecasting applications focus on point predictions, we have emphasized examples that can provide estimates of uncertainty.
When the ML models are able to observe transition phenomena, as in the stochastic case, they performed comparably to MCMC-based forecasting with respect to CRPS and log probability score but under-performed MCMC when there were no transition events in the training sets as in the Hopf and saddle-node examples.
An ensemble forecast combining the predictions of all four ML methods generally scores as well or better than any one of the ML methods alone.
Yet, examining summary statistics, CRPS and log probability scores obscures finer detailed components of the forecasts.
For instance, forecast skill varies with the length of the forecast horizon in a non-monotonic fashion. 
This is the result of multiple factors: for some dynamics, such as those involving tipping points, the long term behavior can be easier to predict than transient transitions. 
Both predicted uncertainty and forecast skill can be better on longer horizons than on shorter ones, as in the MCMC predictions of tipping point dynamics.
It is also important to remember that probabilistic forecast skill scores do not only measure how close observations are to expectation, but also reflect the predicted uncertainty: therefore, over-confidence about predictive accuracy can result in worse scores than scores from forecasts that are less accurate on average but correctly reflect a greater degree of uncertainty.
The ability to better reflect uncertainty rather than better average predictions explains much of the performance of the ensemble model.

The success of these ML models on the stochastic transition case is particularly notable.
All methods are given only a single previous replicate of a stochastic transition (Fig 5, red line) on which to base their estimates.
This is typical of ecological scenarios where data is so often limited.
While even one observation of a transition is more than the methods have in our other forecasts, this still presents a significant challenge to model estimation.
Unlike the MCMC case, the ML models have no prior expectation of a model structure that contains sharp transitions -- we might have expected these models to perform little differently than the ARIMA model. 
Given this single replicate, all four ML models successfully capture the phenomenological pattern of a sharp shift between two stable states -- this is behavior that the structurally simpler family of ARIMA models cannot express.
This provides a clear illustration of the much broader array of phenomenological behaviors that can be accurately modeled with ML models compared to classical statistical models.
In this way, the ML models can be seen as imposing even fewer assumptions on the phenomenological behavior of the system than the ARIMA model.
In contrast, the MCMC performance benefits from very strong process-based assumptions, which happen to match the 'true' model in this case and thus provide a comparison of the theoretical optimal performance.

The MCMC case illustrates some of the hard limits to ecological forecasting of critical transitions.
Our MCMC forecast assumes the _true_ model structure, the data-generating process, is known, and the forecaster need only infer the posterior distribution of model parameters.
This is a much stronger assumption than that made by the ML models, though this assumption can potentially be justified on the basis of a mechanistic understanding of the processes involved.
It is important not to confound the MCMC example here with the use of MCMC in process based models of real systems.
In the real world, this is never the case: all models are at best approximations of the underlying processes [@Oreskes1994]. <!-- ML: maybe "In the real world, having access to the _true_ model is never the case:"-->
Despite this advantage, even the MCMC forecasts differ from the distribution of the true process.
Because the available data come from only a small region of the dynamical state space, they are consistent with many possible parameterizations of the same model structure -- which creates likelihood ridges and non-identifiability of specific parameter values.
Using more simplified versions of the dynamical processes in question, such as the canonical form of a bifurcation, can mitigate this issue in some cases.
Even when such non-identifiability issues cannot be avoided entirely, they can usually be diagnosed by examining the degree of mixing in MCMC sampling and comparing posterior to prior distributions.
<!-- While similar convergence issues may arise in the estimation of ML models, they may be harder to diagnose. -->

When examining the performance of the ML models, it is clear that there is no single method that excels in all scenarios.
Neither is there one class of ML methods that outperforms the others -- a fact we found surprising given the reported dominance of encoder-decoders in the field of sequence-to-sequence deep learning [@aitken_understanding_2021].
These observations underscore the point that ML is a very empirically-driven field in which there are few guarantees on performance.
Furthermore, due to the black-box-ness of deep learning and other reasons like instability to initialization seed, it is often impossible to provide an explanation for why certain methods over-perform or fail to meet expectations.

Overall, ML models and the more traditional ARIMA model fail to predict the qualitative shift in dynamic behavior that occurs in the critical transition scenarios (Hopf and saddle node).
This is not surprising, as the training data provide no prior example of such behavior (e.g. growing oscillations or a sudden shift).
Nevertheless, this should be an important reminder of a central difficulty in ecological forecasting.
Note that in such scenarios, near-term forecasts [@dietze_iterative_2018] may be very accurate right up to the transition event before becoming widely wrong. 
Nor can the possibility of such non-linear behavior be easily dismissed in ecological models -- the examples considered here have been bedrock of ecological modeling and management practices for over half a century [@Folke2004], and if anything are only too simple, representing a small slice of possible dynamical behavior of more complicated models.

It may be natural to ask whether this performance would be remedied if the ML models were trained on data which includes prior examples of supercritical Hopf or saddle node bifurcations. 
This question is not as easy to answer as it may seem, because of the difficulty in defining the corresponding forecasting scenario.
The scenarios we have considered are true, pure forecasts: the training data comes from a single realization of a specific generative process, and the task is to predict the future states of that system before they occur.
Would it be possible to train a predictive algorithm on 'analogous' examples of critical transitions?
For instance, could data from other lakes, which may have experienced a critical transition such as an eutrophication event in the past, be used to train machine learning models to predict such events in some focal lake in the future [@Scheffer2001]?
Perhaps, but it depends on what we mean by an 'analogous' system.
Even if the underlying mechanism was accurately captured by the same model, say, the saddle-node model of @May1977 we consider here, it is likely that most of the individual model parameters would be quite different, even after accounting for re-scaling or non-dimensionalization of the model [@Hastings1996].
Rarely do ecologists have access to completely controlled replicates for fitting or training models.
The ability for ML models to successfully generalize from training in such cases remains an open problem and a promising subject of further investigation.

There are a number of questions that we have left unanswered that we hope will be addressed in future work.
In this paper, we have explored a small number of machine learning and statistical models that can be used for forecasting, so comprehensive conclusions can not be drawn on whether statistical or machine learning-based approaches are better suited for critical transition forecasting problems.
Neither can we claim that the ML methods employed will translate well to all sudden transition event forecasting problems in reality, since working with real data will introduce additional difficulties like how to deal with missing data, sparse data and observation errors.

Furthermore, our analysis has focused on the task of making a single forecast prior to the occurrence of a critical transition.
Forecasting is ideally a more iterative process of data assimilation, where forecasts are updated with respect to additional observations, rather than projecting 100s of time steps into the future [@dietze_iterative_2018]. 
Updating a forecast after a critical transition has already occurred may be of little use in the context of hysteresis, such as under the saddle node or stochastic transition -- recognizing the alternative stable state only after the system is stuck in that basin will often be considered 'too late'.
Assimilation may be more applicable to the Hopf bifurcation, where additional observations of slowly growing oscillations may lead to more accurate forecasts. 
Such models may even accurately predict the homoclinic bifurcation that occurs when the limit cycle grows too large, eventually hitting a saddle point of zero population size for the host species.
We leave these cases to future exploration rather than attempting to explore all such variations in a single narrative. 
We have focused on these illustrative examples accompanied by an extensive appendix of reproducible code implemented using well documented and intuitive frameworks.
We encourage the reader to use these appendices as an entry point into further exploration.

Ecological forecasting is invariably difficult, even in the idealized cases of ample measurement data and clearly identified structural models.
This paper is not intended to give a complete answer to whether deep learning is the best suited method for tipping point forecasting problems as this will take numerous studies to resolve; instead, this paper aims to be an early exploration on whether deep learning methods should be considered as viable tools for this extremely challenging class of prediction problems.
Given the difficulty of forecasting never-before-observed behavior, as illustrated by the Hopf and saddle-node bifurcation scenarios, there is good reason for research to focus more on the kind of qualitative predictions long emphasized in the literature on early warning signals and resilience [@scheffer_anticipating_2012].
Recently, ML techniques developed for classification rather than the ML methods used in regression and forecasting models considered here have demonstrated a more nuanced ability to reliably detect different classes of critical transitions in time-series data [@bury_2021; @lapeyrolerie_teaching_2021].
Rather than seeking to provide managers with quantitative, probabilistic forecasts reflecting a broad uncertainty in possible outcomes, this literature has sought to emphasize only a more qualitative form of prediction, such as establishing whether a system is either "resilient" or "approaching a critical transition."
Decision sciences have long emphasized the importance of reconciling the qualitative predictions of resilience thinking with quantitative forecasts of future states [@Fischer2009; @polasky_decision-making_2011].
Such approaches could be valuable in concert with probabilistic forecasts considered here, providing a possible mechanism to identify when the probabilistic forecast is least reliable.



# Acknowledgements

This material is based upon work supported by the National Science Foundation under Grant No. DBI-1942280. 

# Conflicts of Interest {-}

The authors declare there are no conflicts of interest.

# Authors' Contributions {-}

M.L. and C.B. developed the code and wrote the manuscript.

# Data Availability {-}

We have created a github repository <https://github.com/boettiger-lab/mee_tipping_point_forecasting.> that contains the code used to produce the figures herein.

\newpage

# References
